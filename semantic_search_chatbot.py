# -*- coding: utf-8 -*-
"""Semantic_Search_Chatbot.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1srxgv41ULxr0im5OI5S7MVVJul-4xycx
"""

!pip install sentence-transformers faiss-cpu openai

documents = [
    "Semantic search understands the meaning of text instead of exact keywords.",
    "Embeddings are numerical representations of text meaning.",
    "Vector databases store embeddings and allow similarity search.",
    "Large Language Models generate human-like responses.",
    "RAG combines search results with language models to produce accurate answers."
]

print("Number of documents:", len(documents))

from sentence_transformers import SentenceTransformer

# Load embedding model
model = SentenceTransformer("all-MiniLM-L6-v2")

# Convert documents into embeddings
embeddings = model.encode(documents)

print("Embedding shape:", embeddings.shape)

import faiss
import numpy as np

dimension = embeddings.shape[1]
index = faiss.IndexFlatL2(dimension)

index.add(np.array(embeddings))

print("Vectors stored in database:", index.ntotal)

query = "How does AI understand meaning?"

query_embedding = model.encode([query])

k = 2
distances, indices = index.search(np.array(query_embedding), k)

print("Top matching documents:")
for i in indices[0]:
    print("-", documents[i])

import os
print("API Key Loaded:", bool(os.getenv("OPENAI_API_KEY")))

from google.colab import userdata

api_key = userdata.get("OPENAI_API_KEY")
print("API Key Loaded:", bool(api_key))

from openai import OpenAI
from google.colab import userdata
import numpy as np

# Initialize OpenAI client
client = OpenAI(api_key=userdata.get("OPENAI_API_KEY"))

def rag_chatbot(user_query, k=2):
    # 1. Convert query to embedding
    query_embedding = model.encode([user_query])

    # 2. Search vector database
    distances, indices = index.search(np.array(query_embedding), k)

    # 3. Build context from retrieved documents
    context = "\n".join([documents[i] for i in indices[0]])

    # 4. Prompt LLM with retrieved context
    prompt = f"""
You are a helpful AI assistant.
Answer the question using ONLY the context below.
If the answer is not in the context, say "I don't know based on the given data."

Context:
{context}

Question:
{user_query}
"""

    response = client.chat.completions.create(
        model="gpt-3.5-turbo",
        messages=[{"role": "user", "content": prompt}]
    )

    return response.choices[0].message.content

!pip install transformers accelerate torch

from transformers import pipeline

chat_model = pipeline(
    "text-generation",
    model="google/flan-t5-base",
    max_new_tokens=200
)

def rag_chatbot(user_query, k=2):
    # 1. Embed query
    query_embedding = model.encode([user_query])

    # 2. Vector search
    distances, indices = index.search(np.array(query_embedding), k)

    # 3. Build context
    context = "\n".join([documents[i] for i in indices[0]])

    prompt = f"""
Answer the question using the context below.

Context:
{context}

Question:
{user_query}
Answer:
"""

    result = chat_model(prompt)
    return result[0]["generated_text"]

print(rag_chatbot("What is semantic search?"))